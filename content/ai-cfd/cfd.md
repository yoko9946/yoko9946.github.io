---
title: "AI + CFD!"
date: 2024-06-21
draft: false
---

## Overview
During this program, I gained hands-on experience in **Computational Fluid Dynamics (CFD)**, spanning the fundamentals of PDEs, numerical methods, and solver development, and extending into parallel programming and AI-enhanced approaches. The curriculum provided both theoretical grounding and practical coding exercises.

## Learning Progression
- **Python Foundations**  
  Reviewed Python basics and intermediate concepts as preparation for scientific programming.

- **CFD Fundamentals**  
  Explored governing equations (PDEs), the **Finite Volume Method**, and grid generation techniques.

- **Solving Euler & Navier–Stokes**  
  Implemented numerical solvers for Euler and Navier–Stokes equations on structured grids.

- **Unstructured Grid Methods**  
  Worked with grid generation and solving PDEs on unstructured meshes.

- **Parallel Programming via MPI**  
  Learned domain decomposition and communication strategies for distributed CFD solvers.

- **Parallel Programming via CUDA**  
  Developed GPU kernels, optimized CFD routines, and practiced hardware-agnostic programming.

- **Kokkos Framework**  
  Used Kokkos to build performance-portable CFD code for CPUs and GPUs.

- **Physics-Aware Deep Learning**  
  Integrated machine learning models with CFD, emphasizing physics-informed architectures.

- **Optimization**  
  Applied optimization techniques to improve solver performance and parameter tuning.

## Key Takeaways
- Gained **practical coding skills** in MPI, CUDA, and Kokkos for high-performance CFD.  
- Built understanding of **structured and unstructured mesh solvers** for PDEs.  
- Explored **hybrid AI+CFD workflows**, where deep learning models are constrained by physics.  
- Strengthened ability to translate theoretical models into **scalable implementations**.

## Note on Results
The simulations and performance benchmarks for this program were run on USC’s [Center for Advanced Research Computing (CARC)](https://www.carc.usc.edu/). Due to computing policies and project restrictions, I cannot publicly share the full outputs here, but the experience provided valuable training in **HPC workflows, resource allocation, and cluster-based simulation pipelines**.

